{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924ac134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "def extract_networks(data_dir, dataset, n_minutes=5, original_nets=True):\n",
    "    \"\"\"Function that reads the edgelist (t, i, j) and returns\n",
    "    a network aggregated at n_minutes snapshots as a dictionary of nx.Graph()s,\n",
    "    having t as a key.\n",
    "    If original_nets is set to True it also returns the original non-aggregated network.\"\"\"\n",
    "\n",
    "    # Reading the data and setting t0\n",
    "    f = open(data_dir + '/tij_' + dataset + '.txt')\n",
    "    (t0, i, j) = map(int, str.split(f.readline()))\n",
    "    # Special temporal scale for these two Datasets\n",
    "    if dataset not in ['LyonSchool', 'LH10']:\n",
    "        t0 = t0 * 20\n",
    "    f.close()\n",
    "\n",
    "    # Aggregation on scale of x minutes\n",
    "    delta_t = 20 * 3 * n_minutes\n",
    "    if original_nets == True:\n",
    "        originalnetworks = {}\n",
    "    aggnetworks = {}\n",
    "    f = open(data_dir + '/tij_' + dataset + '.txt')\n",
    "    for line in f:\n",
    "        (t, i, j) = map(int, str.split(line))\n",
    "        # Special temporal scale for these two Datasets\n",
    "        if dataset not in ['LyonSchool', 'LH10']:\n",
    "            t = t * 20\n",
    "        if original_nets == True:\n",
    "            if t not in originalnetworks:\n",
    "                originalnetworks[t] = nx.Graph()\n",
    "            originalnetworks[t].add_edge(i, j)\n",
    "        # this is a trick using the integer division in python\n",
    "        aggtime = t0 + ((t - t0) // delta_t) * delta_t\n",
    "        if aggtime not in aggnetworks:\n",
    "            aggnetworks[aggtime] = nx.Graph()\n",
    "        aggnetworks[aggtime].add_edge(i, j)\n",
    "    f.close()\n",
    "    if original_nets == True:\n",
    "        return originalnetworks, aggnetworks\n",
    "    else:\n",
    "        return aggnetworks\n",
    "\n",
    "def extract_cliques(gs):\n",
    "    listsaggcliques = {}\n",
    "    # looping over the networks in temporal order\n",
    "    for t in sorted(gs.keys()):\n",
    "        listsaggcliques[t] = list(nx.find_cliques(gs[t]))\n",
    "    # returning a dictionary with list of cliques as values\n",
    "    return listsaggcliques\n",
    "\n",
    "def clique_weights(cliques):\n",
    "    from collections import Counter\n",
    "    tot_c = []\n",
    "    for t in cliques:\n",
    "        tot_c.extend(map(frozenset, cliques[t]))\n",
    "    return Counter(tot_c)\n",
    "\n",
    "def clean_non_maximal(ws):\n",
    "    sd = dict(zip(ws.keys(), map(len, ws.keys())))\n",
    "    import operator\n",
    "    sizes = set(map(len, ws.keys()))\n",
    "    sorted_sd = sorted(sd.items(), key=operator.itemgetter(1))\n",
    "    simplices = dict.fromkeys(list(sizes), [])\n",
    "    maximal_simplices = {}\n",
    "    for x in ws:\n",
    "        maximal = True\n",
    "        for xx in ws:\n",
    "            if (len(x) < len(xx)):\n",
    "                if (set(x) < set(xx)):\n",
    "                    maximal = False\n",
    "                    break\n",
    "        if maximal:\n",
    "            maximal_simplices[x] = ws[x]\n",
    "    return maximal_simplices\n",
    "\n",
    "\n",
    "def save_cliques(ws, data_dir, dataset, n_minutes, thr=None):\n",
    "    if thr == None:\n",
    "        ls = map(list, ws.keys())\n",
    "    else:\n",
    "        ls = [list(x) for x in ws if ws[x] >= thr]\n",
    "    jd = open(data_dir + 'aggr_' + str(n_minutes) + 'min_cliques_thr' + str(thr) + '_' + dataset + '.json', 'w')\n",
    "    json.dump(ls, jd)\n",
    "    jd.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c2402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Original_20s/D1.txt', 'r') #Day 1 - School Mid1\n",
    "temporal1 = f.read().splitlines()\n",
    "\n",
    "I=[]\n",
    "J=[]\n",
    "TIJ=[]\n",
    "\n",
    "for line in temporal1[1:len(temporal1)]: #each line in file is i j t_ij delta_t\n",
    "    l=line.split(' ')\n",
    "    I.append(int(l[0])) #ID i\n",
    "    J.append(int(l[1])) #ID j\n",
    "    TIJ.append(int(l[2])) #t_ij beginning interaction\n",
    "    if int(l[3])>1: #delta_t\n",
    "        for k in range(1,int(l[3])): #replicate interaction for delta_t\n",
    "            I.append(int(l[0]))\n",
    "            J.append(int(l[1]))\n",
    "            TIJ.append(int(l[2])+k)\n",
    "\n",
    "f = open('Original_20s/D2.txt', 'r') #Day 2 - School Mid1\n",
    "temporal2 = f.read().splitlines()\n",
    "\n",
    "for line in temporal2[1:len(temporal2)]: #each line in file is i j t_ij delta_t\n",
    "    l=line.split(' ')\n",
    "    I.append(int(l[0])) #ID i\n",
    "    J.append(int(l[1])) #ID j\n",
    "    TIJ.append(int(l[2])+24*60*60/20) #t_ij beginning interaction - following day\n",
    "    if int(l[3])>1: #delta_t\n",
    "        for k in range(1,int(l[3])): #replicate interaction for delta_t\n",
    "            I.append(int(l[0]))\n",
    "            J.append(int(l[1]))\n",
    "            TIJ.append(int(l[2])+k+24*60*60/20) #following day\n",
    "\n",
    "TIJ_sort, I_sort, J_sort = map(list, zip(*sorted(zip(TIJ, I, J)))) #sort interactions according to t_ij\n",
    "\n",
    "f=open('Original_20s/tij_Mid1.txt','w')\n",
    "for i in range(len(TIJ)):\n",
    "    line = \"%i %i %i\\n\"%(TIJ_sort[i], I_sort[i], J_sort[i])\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dd8cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Original_20s/D3.txt', 'r') #Day 1 - School Elem1\n",
    "temporal1 = f.read().splitlines()\n",
    "\n",
    "I=[]\n",
    "J=[]\n",
    "TIJ=[]\n",
    "\n",
    "for line in temporal1[1:len(temporal1)]: #each line in file is i j t_ij delta_t\n",
    "    l=line.split(' ')\n",
    "    I.append(int(l[0])) #ID i\n",
    "    J.append(int(l[1])) #ID j\n",
    "    TIJ.append(int(l[2])) #t_ij beginning interaction\n",
    "    if int(l[3])>1: #delta_t\n",
    "        for k in range(1,int(l[3])): #replicate interaction for delta_t\n",
    "            I.append(int(l[0]))\n",
    "            J.append(int(l[1]))\n",
    "            TIJ.append(int(l[2])+k)\n",
    "\n",
    "f = open('Original_20s/D4.txt', 'r') #Day 2 - School Elem1\n",
    "temporal2 = f.read().splitlines()\n",
    "\n",
    "for line in temporal2[1:len(temporal2)]: #each line in file is i j t_ij delta_t\n",
    "    l=line.split(' ')\n",
    "    I.append(int(l[0])) #ID i\n",
    "    J.append(int(l[1])) #ID j\n",
    "    TIJ.append(int(l[2])+24*60*60/20) #t_ij beginning interaction - following day\n",
    "    if int(l[3])>1: #delta_t\n",
    "        for k in range(1,int(l[3])): #replicate interaction for delta_t\n",
    "            I.append(int(l[0]))\n",
    "            J.append(int(l[1]))\n",
    "            TIJ.append(int(l[2])+k+24*60*60/20) #following day\n",
    "            \n",
    "TIJ_sort, I_sort, J_sort = map(list, zip(*sorted(zip(TIJ, I, J)))) #sort interactions according to t_ij\n",
    "\n",
    "f=open('Original_20s/tij_Elem1.txt','w')\n",
    "for i in range(len(TIJ)):\n",
    "    line = \"%i %i %i\\n\"%(TIJ_sort[i], I_sort[i], J_sort[i])\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c956f414",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'Original_20s/'\n",
    "out_dir = ''\n",
    "\n",
    "datasets = ['Mid1','Elem1']\n",
    "\n",
    "n_minutes = 15\n",
    "thrs = [1]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for thr in thrs:\n",
    "        aggs = extract_networks(dataset_dir, dataset, n_minutes, original_nets=False)\n",
    "        cliques = extract_cliques(aggs)\n",
    "\n",
    "        ws = clique_weights(cliques)\n",
    "        maximal_cliques = clean_non_maximal(ws)\n",
    "        save_cliques(maximal_cliques, out_dir, dataset, n_minutes, thr=thr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
