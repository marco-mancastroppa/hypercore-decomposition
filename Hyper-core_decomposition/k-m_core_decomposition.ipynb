{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f748b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import xgi\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import pickle as pk\n",
    "from itertools import compress\n",
    "\n",
    "dataset='congress-bills_simplices';name_file='{0}'.format(dataset);\n",
    "#dataset='senate-bills_simplices';name_file='{0}'.format(dataset);\n",
    "#dataset='senate-committees_simplices';name_file='{0}'.format(dataset);\n",
    "#dataset='house-committees_simplices';name_file='{0}'.format(dataset);\n",
    "#dataset='email-Enron_simplices'; name_file='{0}'.format(dataset); \n",
    "#dataset='email-Eu_simplices'; name_file='{0}'.format(dataset); \n",
    "#dataset='hyperedges-cat-edge-algebra-questions_simplices'; name_file='{0}'.format(dataset);\n",
    "#dataset='hyperedges-cat-edge-geometry-questions_simplices'; name_file='{0}'.format(dataset);\n",
    "#dataset='hyperedges-cat-edge-music-blues-reviews'; name_file='{0}'.format(dataset);\n",
    "#dataset='Mid1'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='Elem1'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='InVS15'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='SFHH'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='LH10'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='LyonSchool'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='Thiers13'; name_file='aggr_15min_cliques_thr1_{0}'.format(dataset);\n",
    "#dataset='M_PL_015_ECO_ins';name_file='{0}'.format(dataset);\n",
    "#dataset='M_PL_062_ECO_ins';name_file='{0}'.format(dataset);\n",
    "#dataset='M_PL_015_ECO_pl';name_file='{0}'.format(dataset);\n",
    "#dataset='M_PL_015_ECO_pl';name_file='{0}'.format(dataset);\n",
    "\n",
    "with open('Data\\{0}.json'.format(name_file)) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "data = [data[i] for i in range(len(data)) if len(data[i])>1] #select only interactions of size>=2\n",
    "for i in range(len(data)): \n",
    "    data[i].sort() #order the interactions according to the nodes IDs, e.g. [2,1] -> [1,2]\n",
    "data.sort() \n",
    "data = list(data for data,_ in itertools.groupby(data)) #delete fully coincident hyperedges\n",
    "data.sort(key = len)\n",
    "L=len(data)\n",
    "size_max=len(data[L-1])\n",
    "\n",
    "size=list([len(data[j]) for j in range(L)])\n",
    "\n",
    "X = xgi.Hypergraph(data) #build the hypergraph\n",
    "IDX = list(X.nodes)\n",
    "IDX_e = list(X.edges) \n",
    "N=len(IDX)\n",
    "\n",
    "M=range(2,size_max+1) \n",
    "k_step=1\n",
    "K=range(0,1200,k_step) \n",
    "\n",
    "k_shell_dict={}\n",
    "idx_orig = IDX\n",
    "\n",
    "IDX_size = range(len(size))\n",
    "k_max=np.zeros(len(M))\n",
    "\n",
    "for j in idx_orig:\n",
    "    k_shell_dict[j]=np.zeros(len(M))\n",
    "\n",
    "for x in range(len(M)):\n",
    "    m=M[x]  \n",
    "    \n",
    "    D=np.zeros(len(K))  \n",
    "\n",
    "    idx_size = list(compress(IDX_size, np.greater_equal(size, m * np.ones(len(size))))) #consider only hyperedges of size >=m\n",
    "    int_sel = list([data[i] for i in idx_size])\n",
    "    X = xgi.Hypergraph(int_sel) #build hypergraph with only interactions of size >=m\n",
    "    IDX = list(X.nodes) \n",
    "    IDX_e = list(X.edges) \n",
    "\n",
    "    for y in range(len(K)):\n",
    "        kk = K[y]\n",
    "        \n",
    "        d_tot_m = np.zeros(len(IDX))\n",
    "        prev_shell=IDX\n",
    "            \n",
    "        for i in range(len(IDX)):\n",
    "            d_tot_m[i] = X.degree(IDX[i])\n",
    "        \n",
    "        idx_n_remove = list(compress(IDX, np.greater(kk * np.ones(len(d_tot_m)),d_tot_m))) #nodes with degree<k are removed\n",
    "        X.remove_nodes_from(idx_n_remove)\n",
    "        IDX = list(X.nodes)\n",
    "        IDX_e = list(X.edges)\n",
    "                \n",
    "        sizes = [val for (edge, val) in X.edge_size()] #hyperedges with size <m are removed \n",
    "        idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i]<m] \n",
    "        X.remove_edges_from(idx_e_remove)\n",
    "        \n",
    "        int_sel = [list(e) for e in X._edge.values()] #fully coincident hyperedges are removed\n",
    "        int_sel.sort() \n",
    "        int_sel = list(int_sel for int_sel, _ in itertools.groupby(int_sel))\n",
    "        X = xgi.Hypergraph(int_sel) #build new hypergraph\n",
    "        IDX = list(X.nodes)\n",
    "        IDX_e = list(X.edges)\n",
    "\n",
    "        while len(idx_n_remove)>0 or len(idx_e_remove)>0: \n",
    "            \n",
    "            d_tot_m = np.zeros(len(IDX))  \n",
    "            \n",
    "            for i in range(len(IDX)):\n",
    "                d_tot_m[i] = X.degree(IDX[i])\n",
    "                \n",
    "            idx_n_remove = list(compress(IDX, np.greater(kk * np.ones(len(d_tot_m)),d_tot_m))) #nodes with degree<k are removed\n",
    "            X.remove_nodes_from(idx_n_remove)\n",
    "            IDX = list(X.nodes)\n",
    "            IDX_e = list(X.edges)\n",
    "\n",
    "            sizes = [val for (edge, val) in X.edge_size()] #hyperedges with size <m are removed \n",
    "            idx_e_remove = [IDX_e[i] for i in range(len(IDX_e)) if sizes[i]<m]\n",
    "            X.remove_edges_from(idx_e_remove)\n",
    "            \n",
    "            int_sel = [] #fully coincident hyperedges are removed\n",
    "            int_sel = [list(e) for e in X._edge.values()]\n",
    "            int_sel.sort()\n",
    "            int_sel = list(int_sel for int_sel, _ in itertools.groupby(int_sel))\n",
    "            X = xgi.Hypergraph(int_sel)\n",
    "            IDX = list(X.nodes)\n",
    "            IDX_e = list(X.edges)\n",
    "            \n",
    "        shell_kk = list(sorted(set(prev_shell) - set(IDX)))\n",
    "        for j in shell_kk:\n",
    "            k_shell_dict[j][x]=kk-k_step\n",
    "\n",
    "        D[y] = len(X.nodes)\n",
    "        if y > 0:\n",
    "            if D[y] == 0 and D[y - 1] != 0:\n",
    "                k_max[x] = kk - k_step #maximum connectivity at order m\n",
    "        if D[y]==0:\n",
    "            break #stop the decomposition when the (k,m)-core is empty\n",
    "            \n",
    "pk.dump(k_shell_dict, open('K-shell_dict_{0}_hyper.pck'.format(dataset),'wb')) #m-shell index\n",
    "\n",
    "with open('K-max_{0}_hyper.json'.format(dataset),'w') as f:\n",
    "    json.dump(k_max.tolist(), f) #maximum connectivity\n",
    "    \n",
    "R_dict={} #size-independent hypercoreness\n",
    "for y in k_shell_dict:\n",
    "    R_dict[y]=sum(np.array(k_shell_dict[y])/np.array(k_max))\n",
    "\n",
    "pk.dump(R_dict, open('R_dict_{0}_hyper.pck'.format(dataset),'wb'))\n",
    "\n",
    "Psi=[] #Psi(m)  distribution of hyperedges size\n",
    "for m in range(2,size_max+1):\n",
    "    Psi.append(size.count(m)/len(size))\n",
    "\n",
    "R_w_dict={} #frequency-based hypercoreness\n",
    "for y in k_shell_dict:\n",
    "    R_w_dict[y]=sum(np.array(Psi)*np.array(k_shell_dict[y])/np.array(k_max))\n",
    "    \n",
    "pk.dump(R_w_dict, open('R_w_dict_{0}_hyper.pck'.format(dataset),'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
